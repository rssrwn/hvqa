\documentclass[../interim.tex]{subfiles}


\begin{document}

\section{Related Work} \label{section:related}

\subsection{VideoQA Datasets}

A number of datasets are available for the VideoQA problem, in this section we discuss each of the available datasets.

The MovieQA dataset~\cite{dataset:movie-qa} is a VideoQA dataset consisting of 14,944 multiple-choice questions which align with video clips from movies. The clips come from a collection of 408 movies and the Question-Answer (QA) pairs were generated by humans in three steps. Firstly, one annotator would be given a plot summary for a moive and asked to create correct QA pair, additionally this annotator would mark the area of the plot summary to which their QA pair corresponded. Next, a different annotator would be asked to provide four incorrect answers to a QA pair (and was given the option to correct the original question and answer). Finally, annotators would align each sentence in the plot summary to the video by marking the start and end points, this allowed each QA pair to be aligned with a video clip. The questions and anwers are written in free-form natural language.

Zeng et al.~\cite{dataset:zeng} create a much larger VideoQA dataset by automatically generating QA pairs from videos and associated descriptions collected online. Their dataset consists of 18100 videos as well 151263 and 21352 automatically generated QA pairs in the training and validation sets, respectively. The dataset also contains 2461 human-generated QA pairs to be used for testing. Their questions and answers are free-form natural language, however, a large number of their answers are yes and no (32.5\% and 32.5\%, respectively).

The TGIF-QA dataset~\cite{dataset:tgif-qa} is commonly used for assessing the performance of VideoQA models. The updated version of the TGIF-QA dataset contains human-generated 165,165 QA pairs collected from 71,741 GIFs, sourced from the TGIF dataset~\cite{dataset:tgif}, which contains a number of GIFs and associated descriptions. There are four possible types of questions in the TGIF-QA dataset, three specific to VideoQA, requiring temporal knowledge to answer, and the last asks questions related to a single frame (VQA). The question types are as follows:
\begin{itemize}
  \item \textbf{Repetition Count}. Counting the number of repetitions of an action. There are 11 possible answers (0 to 10+).

  \item \textbf{Repeating Action}. A multiple-choice question about identifying an action that has been repeated in the video. For example, \textbf{Q}: What does the duck do three times? \textbf{A}: Shake head.

  \item \textbf{State Transition}. A multiple-choice question about identifying the state before or after another state. For example, \textbf{Q}: What does the bear do after sitting? \textbf{A}: Stand.

  \item \textbf{FrameQA}. Open-ended questions related to a single frame.
\end{itemize}

For the VideoQA questions the authors created templates for questions and used a large number of human annotators to speed up the generation process. The FrameQA questions are generated using the descriptions from the TGIF dataset. A number of quality control checks were also included.


\end{document}
