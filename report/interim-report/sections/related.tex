\documentclass[../interim.tex]{subfiles}


\begin{document}

\section{Related Work} \label{section:related}

\subsection{VideoQA Datasets}

A number of datasets are available for the VideoQA problem, in this section we discuss each of the available datasets.

The MovieQA dataset~\cite{dataset:movie-qa} is a VideoQA dataset consisting of 14,944 multiple-choice questions which align with video clips from movies. The clips come from a collection of 408 movies and the Question-Answer (QA) pairs were generated by humans in three steps. Firstly, one annotator would be given a plot summary for a moive and asked to create correct QA pair, additionally this annotator would mark the area of the plot summary to which their QA pair corresponded. Next, a different annotator would be asked to provide four incorrect answers to a QA pair (and was given the option to correct the original question and answer). Finally, annotators would align each sentence in the plot summary to the video by marking the start and end points, this allowed each QA pair to be aligned with a video clip. The questions and anwers are written in free-form natural language.

Zeng et al.~\cite{dataset:zeng} create a much larger VideoQA dataset by automatically generating QA pairs from videos and associated descriptions collected online. Their dataset consists of 18100 videos as well 151263 and 21352 automatically generated QA pairs in the training and validation sets, respectively. The dataset also contains 2461 human-generated QA pairs to be used for testing. Their questions and answers are free-form natural language, however, a large number of their answers are yes and no (32.5\% and 32.5\%, respectively).

The TGIF-QA dataset~\cite{dataset:tgif-qa} is commonly used for assessing the performance of VideoQA models. The updated version of the TGIF-QA dataset contains human-generated 165,165 QA pairs collected from 71,741 GIFs, sourced from the TGIF dataset~\cite{dataset:tgif}, which contains a number of GIFs and associated descriptions. There are four possible types of questions in the TGIF-QA dataset, three specific to VideoQA, requiring temporal knowledge to answer, and the last asks questions related to a single frame (VQA). The question types are as follows:
\begin{itemize}
  \item \textbf{Repetition Count}. Counting the number of repetitions of an action. There are 11 possible answers (0 to 10+).

  \item \textbf{Repeating Action}. A multiple-choice question about identifying an action that has been repeated in the video. For example, \textbf{Q}: What does the duck do three times? \textbf{A}: Shake head.

  \item \textbf{State Transition}. A multiple-choice question about identifying the state before or after another state. For example, \textbf{Q}: What does the bear do after sitting? \textbf{A}: Stand.

  \item \textbf{FrameQA}. Open-ended questions related to a single frame.
\end{itemize}

For the VideoQA questions the authors created templates for questions and used a large number of human annotators to speed up the generation process. The FrameQA questions are generated using the descriptions from the TGIF dataset. A number of quality control checks were also included.

Zhu et al.~\cite{dataset:zhu} have proposed a a VideoQA dataset containing fill-in-the-blank style questions, with multiple-choice answers. The dataset contains over 100,000 real-world video clips and 400,000 questions. The dataset is generated from three different annotated video sources. On top of questions which ask the model to describe the present (describe the current video), for two of three video sources the authors also introduce two additional question types: infer the past and predict the future. For these two types of questions the model is asked a question on a part of the video which it is not explicitly given; these questions require the model to use some form of commonsense reasoning to generate a correct answer. One of the advantages of using a multiple-choice dataset such as this is that it is more amenable to quantitative evaluation than datasets with free-form answers, since answers are either right or wrong.

The EgoVQA dataset~\cite{dataset:ego-vqa} attempts to address the lack of first-person VideoQA datasets. The dataset contains 581 QA pairs with both multiple-choice questions (with 5 possible answers per question) and open-ended questions. The dataset was generated by manually generating QA pairs from a pre-existing set of 16 first-person videos. The authors also show that existing VideoQA models only marginally outperformed random choice on questions related to the colour of objects. They conjecture that existing models struggle to separate attentions on camera wearers from attentions on third persons.

\end{document}
